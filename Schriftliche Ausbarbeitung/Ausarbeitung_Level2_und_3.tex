\documentclass{report}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{enumerate}% schicke Nummerierung
\usepackage{graphicx}
\usepackage[english, ngerman]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage{bigdelim}
\usepackage{multirow}
\usepackage{dsfont, hyperref}
\usepackage{cite}
\usepackage[nottoc]{tocbibind}
\usepackage{empheq}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{lipsum}
\geometry{a4paper,left=40mm,right=30mm, top=5cm, bottom=5cm} 



\newtheoremstyle{linebreak}   % name
{3pt}                         % Space above
{3pt}                         % Space below
{}                            % Body font
{}                            % Indent amount 1
{\bfseries}                   % Theorem head font
{\newline}                    % Punctuation after theorem head
{.5em}                        % Space after theorem head 2
{}                            % Theorem head spec (can be left empty, meaning ‘normal’)
\theoremstyle{linebreak}
\newtheorem{defi}{Definition}%[chapter]
\newtheorem{satz}[defi]{Satz}
\newtheorem{theorem}[defi]{Theorem}
\newtheorem{propo}[defi]{Proposition}
\newtheorem{lemma}[defi]{Lemma}
\newtheorem{cor}[defi]{Korollar}
\newtheorem{bem}[defi]{Bemerkung}
\newtheorem{bsp}[defi]{Beispiel}
\newtheorem{folg}[defi]{Folgerung}
%bemerkungen oder Fließtext???
 \newcommand{\newln}{\\&\quad\quad{}}

\renewenvironment{abstract}
 {\small
  \begin{center}
  \bfseries \abstractname\vspace{-.5em}\vspace{0pt}
  \end{center}
  \list{}{%
    \setlength{\leftmargin}{12mm}% <---------- CHANGE HERE
    \setlength{\rightmargin}{\leftmargin}%
  }%
  \item\relax}
 {\endlist}




\begin{document}
\title{Seminar über Numerische Methoden im Machine Learning \\ Bayesianische Inferenz}

\author{Daniel Luft & Fabian Gernandt \\ Prof. Dr. V. Schulz}
%\maketitle

%\tableofcontents 
\selectlanguage{ngerman}

\subsection{Hyperparameterschätzung \& Kernelselektion}

Nachdem wir nun in dem vorangehenden Kapitel gesehen haben wie man die LS-SVM und ihr duales Problem herleitet, knüpfen wir darauf aufbauend an und leiten mögliche Arten der Hyperparameterschätzung und der Kernelselektion her. 

Die Bayesianische Statistik ermöglicht eine rigorose Behandlung von fast beliebig komplexen statistischen Modellen durch A Priori Annahmen an Hyperparameter und deren Parameterabhängigkeit. Wir folgen weiterhin dem Ansatz aus \cite{LS-SVM} das Problem der Hyperparameterinferenz durch das Nutzen der so entstehenden MAP-Schätzer herzuleiten.

Zunächst betrachten wir erneut unsere LS-SVM Gleichung:

\begin{center}
	$(\hat{w}_{MAP},\hat{b}_{MAP})  = \underset{w,b}{\arg \min \hspace{0.1cm}} 					 \frac{\mu}{2}w^Tw + \frac{\xi}{2}\underset{i=1}{\overset{n}{\sum}}e_i^2$
	unter $e_i = 1 - y_i(w^T \phi(x_i) +b)$
\end{center}

Wir machen folgende Annahmen an die Verteilung der Hyperparemter $\mu, \xi >0$:

\begin{itemize}
	\item $\log(\mu) \sim \mathcal{N}(0, \sigma_\mu^2), \log(\xi) \sim \mathcal{N}(0, 				  \sigma_\xi^2)$ 
	\item $\log(\mu), \log(\xi)$ stochastisch unabhängig
	\item $\sigma_\mu^2, \sigma_\xi^2 \rightarrow \infty$.
\end{itemize}

Die Annahmen sind dadurch zu rechtfertigen, dass als log-normalverteilte Zufallsvariablen die positiven Werte von $\mu$ und $\xi$ respektiert werden, der Grenzwert der Varianzen $\sigma_\mu^2, \sigma_\xi^2 \rightarrow \infty$ dadurch, dass man eine uniforme Annahme über die Verteilung von $\mu$ und $\xi$ machen möchte, d.h. keine Vorinformation über ihre Werte verwenden will. Weiterhin wird die stochastische Unabhängigkeit die rechnerische Herleitung des Minimierungsproblems ermöglichen.

Zunächst stellen wir über den Posterior $\mathbb{P}(\log \mu, \log \xi \vert D, K)$ mit Hilfe der Annahmen fest, dass

\begin{align*}
		& \mathbb{P}(\log(\mu), \log(\xi)\vert D, K)\\
	   =& \frac{\mathbb{P}(D\vert \log(\mu),\log(\xi),K)\mathbb{P}(\log(\mu), \log(\xi) 			  \vert K)}{\mathbb{P}(D\vert K)}  \\
\propto & \mathbb{P}(D\vert \log(\mu),\log(\xi),K)\exp(-\frac{x^2}{2\sigma_							  \mu^2})\exp(-\frac{x^2}{2\sigma_\xi^2})\\
\rightarrow & \mathbb{P}(D\vert \log(\mu),\log(\xi),K).
\end{align*}

Somit reicht es zum Aufstellen des MAP-Schätzers die Likelihood  $\mathbb{P}(D\vert \log(\mu),\log(\xi),K)$ gegeben der Hyperparameter $\mu, \xi$ zu betrachten. Diese Likelihood, und damit $\mathbb{P}(\log(\mu), \log(\xi)\vert D, K)$, lässt sich weiter mit Hilfe der MAP-Schätzer $(\hat{w}_{MAP}, \hat{b}_{MAP})$ darstellen, was wir in folgendem Lemma zusammenfassen.

\newpage

\begin{lemma}[Darstellung der A Postiori Wahrscheinlichkeit]

Es gelten die zu Beginn des Kapitels getroffenen Annahmen an $\mu, \xi$. Dann folgt

\begin{align*}
	\mathbb{P}(\log(\mu), \log(\xi)\vert D, K) \propto & \mathbb{P}(D\vert \log(\mu),			\log(\xi),K) \\
	\propto & \frac{\sqrt{\mu^{n_f} \xi^n}}	{\sqrt{\det H}} \exp(- \mathcal{J}(\hat{w}			_{MAP},\hat{b}_{MAP})),
\end{align*}

wobei 
\begin{center}
	$\mathcal{J}(w,b) = \frac{\mu}{2}w^T w + \frac{\xi}{2}\overset{n}{\underset{i=1}			{\sum}} e_i ^2 \text{ und } H = \begin{pmatrix}
	\frac{\partial^2 \mathcal{J}}{\partial w^2 } & \frac{\partial^2 \mathcal{J}}				{\partial w \partial b } \\
	\frac{\partial^2 \mathcal{J}}{\partial b \partial w } & \frac{\partial^2 					\mathcal{J}}{\partial b^2 }
	\end{pmatrix}.$
\end{center}

\end{lemma}
%VERWEIS AUF VORIGES KAPITEL, GLEICHUNGEN WIE IN DEM LEVELS PAPER

\begin{proof}
Die erste Proportionalität wurde vor Beginn des Lemmas gezeigt, für die zweite siehe \cite{LS-SVM}.
\end{proof}

An dieser Stelle wäre es schon möglich das Optimierungsproblem für die Hyperparameterinferenz zu formulieren, was aufgrund der Determinante $\det H$ zu keiner befriedigenden Ausdruck führt. Dem entgegnen Gestel, Suykens et. al. durch folgende Darstellung der Determinante:

\begin{lemma}[Darstellung der Determinante der Hessematrix]
Betrache die aus dem LS-SVM Problem stammenden Ausdrücke 
\begin{center}
	$\mathcal{J}(w,b) = \frac{\mu}{2}w^T w + \frac{\xi}{2}\overset{n}{\underset{i=1}			{\sum}} e_i ^2 \text{ und } H = \begin{pmatrix}
	\frac{\partial^2 \mathcal{J}}{\partial w^2 } & \frac{\partial^2 \mathcal{J}}				{\partial w \partial b } \\
	\frac{\partial^2 \mathcal{J}}{\partial b \partial w } & \frac{\partial^2 					\mathcal{J}}{\partial b^2 }
	\end{pmatrix}.$
\end{center}

Dann gilt

\begin{center}
	$\det(H) = n \mu^{n_f - N_{eff}} \xi \underset{i=1}{\overset{N_{eff}}{\prod}}(\mu + 			\xi \lambda_i)$,
\end{center}

wobei $N_{eff}$ die Anzahl der Eigenwerte $\lambda_i$ ungleich Null der zentrierten Matrix $M \Omega M$ mit
\begin{center}
	$\Omega_{i,j} = K(x_i, x_j)$ und $M = I_n + \frac{1}{n} 1_v 1_v^T$ ist.
\end{center}
\end{lemma}

\begin{proof}
Siehe \cite{LS-SVM} Appendix B.
\end{proof}

Beide darstellenden Lemmas lassen sich nun bei einem MAP-Schätzer Ansatz kombinieren und ergeben eines der Hauptresultate zur LS-SVM:


\begin{theorem}[Hyperparameterinferenz der LS-SVM]

Seien die Voraussetzungen an a priori Verteilungen wie zu Beginn des Kapitels.\\
(i) Dann sind die MAP-Schätzer der Hyperparameter $\mu, \xi$ gegeben durch

\begin{align*}
	(\hat{\mu}_{MAP}, \hat{\xi}_{MAP}) =& \underset{\mu, \xi}{\arg \min}  \hspace{0.2cm}		\mathcal{J}(\hat{w}_{MAP}, \hat{b}_{MAP}) + \frac{1}{2}\sum_{i=1}^{N_{eff}}\log(\mu 		+ \xi \lambda_i) \\
	 & \hspace{0.8cm}- \frac{N_{eff}}{2}\log(\mu) - \frac{n -1 }{2}\log(\xi)\\
	=:& \underset{\mu, \xi}{\arg \min} \hspace{0.2cm}	\mathcal{J}_\text{hyp}(\mu, \xi)
\end{align*}



mit dem Ausgangsfunktional der LS-SVM 
$\mathcal{J}(w,b) = \frac{\mu}{2}w^T w + \frac{\xi}{2}\overset{n}{\underset{i=1}{\sum}} e_i ^2$,
sowie den nichttrivialen Eigenwerten $\lambda_i$ der zentrierten Kernelmatrix.

(ii) Weiterhin sind die partiellen Ableitungen des Zielfunktionals gegeben durch

\begin{align*}
	\frac{\partial \mathcal{J}_\text{hyp}}{\partial \mu} = & \hat{w}_{MAP}^T \hat{w}			_{MAP} + \frac{1}{2} \overset{N_{eff}}{\underset{i=1}{\sum}} \frac{1}{\mu + \xi 			\lambda_i} - \frac{N_{eff}}{2\mu} \\
    \frac{\partial \mathcal{J}_\text{hyp}}{\partial \xi} = & \overset{n}						{\underset{i=1}{\sum}}( y_i - (\hat{w}_{MAP}^T \varphi(x_i) + \hat{b}_{MAP}))^2 + \overset{N_{eff}}{\underset{i=1}{\sum}} \frac{\lambda_i}{\mu + \xi \lambda_i} - \frac{N-1}{2\xi}
\end{align*} 


(iii) Schließlich gilt im Optimum folgende Relation:


\end{theorem}



\nocite{*}
\bibliographystyle{plain}
\bibliography{papers}

\end{document}
