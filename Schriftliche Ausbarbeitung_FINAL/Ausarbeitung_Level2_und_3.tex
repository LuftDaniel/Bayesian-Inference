\documentclass{report}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{enumerate}% schicke Nummerierung
\usepackage{graphicx}
\usepackage[english, ngerman]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage{bigdelim}
\usepackage{multirow}
\usepackage{dsfont, hyperref}
\usepackage{cite}
\usepackage[nottoc]{tocbibind}
\usepackage{empheq}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{lipsum}
\geometry{a4paper,left=40mm,right=30mm, top=5cm, bottom=5cm} 



\newtheoremstyle{linebreak}   % name
{3pt}                         % Space above
{3pt}                         % Space below
{}                            % Body font
{}                            % Indent amount 1
{\bfseries}                   % Theorem head font
{\newline}                    % Punctuation after theorem head
{.5em}                        % Space after theorem head 2
{}                            % Theorem head spec (can be left empty, meaning ‘normal’)
\theoremstyle{linebreak}
\newtheorem{defi}{Definition}%[chapter]
\newtheorem{satz}[defi]{Satz}
\newtheorem{theorem}[defi]{Theorem}
\newtheorem{propo}[defi]{Proposition}
\newtheorem{lemma}[defi]{Lemma}
\newtheorem{cor}[defi]{Korollar}
\newtheorem{bem}[defi]{Bemerkung}
\newtheorem{bsp}[defi]{Beispiel}
\newtheorem{folg}[defi]{Folgerung}
%bemerkungen oder Fließtext???
 \newcommand{\newln}{\\&\quad\quad{}}

\renewenvironment{abstract}
 {\small
  \begin{center}
  \bfseries \abstractname\vspace{-.5em}\vspace{0pt}
  \end{center}
  \list{}{%
    \setlength{\leftmargin}{12mm}% <---------- CHANGE HERE
    \setlength{\rightmargin}{\leftmargin}%
  }%
  \item\relax}
 {\endlist}




\begin{document}
\title{Seminar über Numerische Methoden im Machine Learning \\ Bayesianische Inferenz}

\author{Daniel Luft & Fabian Gernandt \\ Prof. Dr. V. Schulz}
%\maketitle

%\tableofcontents 
\selectlanguage{ngerman}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Algorithmen zur Lösung???

%Schreibweise: Kernel oder Kerne??

%SKALARES PROBLEM?

%Verweis auf voriges Kapitel + Klärung der Begriffe n_f, N_eff nicht vergessen!

%Ausblick   + Algorithmus (evtl extra Kapitel mit sachen zur implementierung und bildern)

%diagramme nicht vergessen
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Hyperparameterschätzung \& Kernelselektion}

Nachdem wir nun in dem vorangehenden Kapitel gesehen haben wie man die LS-SVM und ihr duales Problem herleitet, knüpfen wir darauf aufbauend an und leiten mögliche Arten der Hyperparameterschätzung und der Kernelselektion her. 

Die Bayesianische Statistik ermöglicht eine rigorose Behandlung von fast beliebig komplexen statistischen Modellen durch A Priori Annahmen an Hyperparameter und deren Parameterabhängigkeit. Wir folgen weiterhin dem Ansatz aus \cite{LS-SVM} das Problem der Hyperparameterinferenz durch das Nutzen der so entstehenden MAP-Schätzer herzuleiten.

Zunächst betrachten wir erneut unsere LS-SVM Gleichung:

\begin{center}
	$(\hat{w}_{MAP},\hat{b}_{MAP})  = \underset{w,b}{\arg \min \hspace{0.1cm}} 					 \frac{\mu}{2}w^Tw + \frac{\xi}{2}\underset{i=1}{\overset{n}{\sum}}e_i^2$
	unter $e_i = 1 - y_i(w^T \phi(x_i) +b)$
\end{center}

Wir machen folgende Annahmen an die Verteilung der Hyperparemter $\mu, \xi >0$:

\begin{itemize}
	\item $\log(\mu) \sim \mathcal{N}(0, \sigma_\mu^2), \log(\xi) \sim \mathcal{N}(0, 				  \sigma_\xi^2)$ 
	\item $\log(\mu), \log(\xi)$ stochastisch unabhängig
	\item $\sigma_\mu^2, \sigma_\xi^2 \rightarrow \infty$.
\end{itemize}

Die Annahmen sind dadurch zu rechtfertigen, dass als log-normalverteilte Zufallsvariablen die positiven Werte von $\mu$ und $\xi$ respektiert werden, der Grenzwert der Varianzen $\sigma_\mu^2, \sigma_\xi^2 \rightarrow \infty$ dadurch, dass man eine uniforme Annahme über die Verteilung von $\mu$ und $\xi$ machen möchte, d.h. keine Vorinformation über ihre Werte verwenden will. Weiterhin wird die stochastische Unabhängigkeit die rechnerische Herleitung des Minimierungsproblems ermöglichen.

Zunächst stellen wir über den Posterior $\mathbb{P}(\log \mu, \log \xi \vert D, K)$ mit Hilfe der Annahmen fest, dass

\begin{align*}
		& \mathbb{P}(\log(\mu), \log(\xi)\vert D, K)\\
	   =& \frac{\mathbb{P}(D\vert \log(\mu),\log(\xi),K)\mathbb{P}(\log(\mu), \log(\xi) 			  \vert K)}{\mathbb{P}(D\vert K)}  \\
\propto & \mathbb{P}(D\vert \log(\mu),\log(\xi),K)\exp(-\frac{x^2}{2\sigma_							  \mu^2})\exp(-\frac{x^2}{2\sigma_\xi^2})\\
\rightarrow & \mathbb{P}(D\vert \log(\mu),\log(\xi),K).
\end{align*}

Somit reicht es zum Aufstellen des MAP-Schätzers die Likelihood  $\mathbb{P}(D\vert \log(\mu),\log(\xi),K)$ gegeben der Hyperparameter $\mu, \xi$ zu betrachten. Diese Likelihood, und damit $\mathbb{P}(\log(\mu), \log(\xi)\vert D, K)$, lässt sich weiter mit Hilfe der MAP-Schätzer $(\hat{w}_{MAP}, \hat{b}_{MAP})$ darstellen, was wir in folgendem Lemma zusammenfassen.

\newpage

\begin{lemma}[Darstellung der A Postiori Wahrscheinlichkeit]

Es gelten die zu Beginn des Kapitels getroffenen Annahmen an $\mu, \xi$. Dann folgt

\begin{align*}
	\mathbb{P}(\log(\mu), \log(\xi)\vert D, K) \propto & \mathbb{P}(D\vert \log(\mu),			\log(\xi),K) \\
	\propto & \frac{\sqrt{\mu^{n_f} \xi^n}}	{\sqrt{\det H}} \exp(- \mathcal{J}(\hat{w}			_{MAP},\hat{b}_{MAP})),
\end{align*}

wobei 
\begin{center}
	$\mathcal{J}(w,b) = \frac{\mu}{2}w^T w + \frac{\xi}{2}\overset{n}{\underset{i=1}			{\sum}} e_i ^2 \text{ und } H = \begin{pmatrix}
	\frac{\partial^2 \mathcal{J}}{\partial w^2 } & \frac{\partial^2 \mathcal{J}}				{\partial w \partial b } \\
	\frac{\partial^2 \mathcal{J}}{\partial b \partial w } & \frac{\partial^2 					\mathcal{J}}{\partial b^2 }
	\end{pmatrix}.$
\end{center}

\end{lemma}
%VERWEIS AUF VORIGES KAPITEL, GLEICHUNGEN WIE IN DEM LEVELS PAPER

\begin{proof}
Die erste Proportionalität wurde vor Beginn des Lemmas gezeigt, für die zweite siehe \cite{LS-SVM}.
\end{proof}

An dieser Stelle wäre es schon möglich das Optimierungsproblem für die Hyperparameterinferenz zu formulieren, was aufgrund der Determinante $\det H$ zu keiner befriedigenden Ausdruck führt. Dem entgegnen Gestel, Suykens et. al. durch folgende Darstellung der Determinante:

\begin{lemma}[Darstellung der Determinante der Hessematrix]
Betrache die aus dem LS-SVM Problem stammenden Ausdrücke 
\begin{center}
	$\mathcal{J}(w,b) = \frac{\mu}{2}w^T w + \frac{\xi}{2}\overset{n}{\underset{i=1}			{\sum}} e_i ^2 \text{ und } H = \begin{pmatrix}
	\frac{\partial^2 \mathcal{J}}{\partial w^2 } & \frac{\partial^2 \mathcal{J}}				{\partial w \partial b } \\
	\frac{\partial^2 \mathcal{J}}{\partial b \partial w } & \frac{\partial^2 					\mathcal{J}}{\partial b^2 }
	\end{pmatrix}.$
\end{center}

Dann gilt

\begin{center}
	$\det(H) = n \mu^{n_f - N_{eff}} \xi \underset{i=1}{\overset{N_{eff}}{\prod}}(\mu + 			\xi \lambda_i)$,
\end{center}

wobei $N_{eff}$ die Anzahl der Eigenwerte $\lambda_i$ ungleich Null der zentrierten Matrix $M \Omega M$ mit
\begin{center}
	$\Omega_{i,j} = K(x_i, x_j)$ und $M = I_n + \frac{1}{n} 1_v 1_v^T$ ist.
\end{center}
\end{lemma}

\begin{proof}
Siehe \cite{LS-SVM} Appendix B.
\end{proof}

Beide darstellenden Lemmas lassen sich nun bei einem MAP-Schätzer Ansatz kombinieren und ergeben eines der Hauptresultate zur LS-SVM:


\begin{theorem}[Hyperparameterinferenz der LS-SVM]

Seien die Voraussetzungen an a priori Verteilungen wie zu Beginn des Kapitels.\\
(i) Dann sind die MAP-Schätzer der Hyperparameter $\mu, \xi$ gegeben durch

\begin{align*}
	(\hat{\mu}_{MAP}, \hat{\xi}_{MAP}) =& \underset{\mu, \xi}{\arg \min}  \hspace{0.2cm}		\mathcal{J}(\hat{w}_{MAP}, \hat{b}_{MAP}) + \frac{1}{2}\sum_{i=1}^{N_{eff}}\log(\mu 		+ \xi \lambda_i) \\
	 & \hspace{0.8cm}- \frac{N_{eff}}{2}\log(\mu) - \frac{n -1 }{2}\log(\xi)\\
	=:& \underset{\mu, \xi}{\arg \min} \hspace{0.2cm}	\mathcal{J}_\text{hyp}(\mu, \xi)
\end{align*}



mit dem Ausgangsfunktional der LS-SVM 
$\mathcal{J}(w,b) = \frac{\mu}{2}w^T w + \frac{\xi}{2}\overset{n}{\underset{i=1}{\sum}} e_i ^2$,
sowie den nichttrivialen Eigenwerten $\lambda_i$ der zentrierten Kernelmatrix.

(ii) Weiterhin sind die partiellen Ableitungen des Zielfunktionals gegeben durch

\begin{align*}
	\frac{\partial \mathcal{J}_\text{hyp}}{\partial \mu} = & \hat{w}_{MAP}^T \hat{w}			_{MAP} + \frac{1}{2} \overset{N_{eff}}{\underset{i=1}{\sum}} \frac{1}{\mu + \xi 			\lambda_i} - \frac{N_{eff}}{2\mu} \\
    \frac{\partial \mathcal{J}_\text{hyp}}{\partial \xi} = & \overset{n}						{\underset{i=1}{\sum}}( y_i - (\hat{w}_{MAP}^T \varphi(x_i) + \hat{b}_{MAP}))^2 + \overset{N_{eff}}{\underset{i=1}{\sum}} \frac{\lambda_i}{\mu + \xi \lambda_i} - \frac{N-1}{2\xi}.
\end{align*} 
\end{theorem}

\begin{proof}
Wir verwenden die beiden vorausgegangenen Lemmata und erhalten durch einsetzen und der Definition des MAP-Schätzers, sowie negativer log-Transformation:
\begin{align*}
	(\hat{\mu}_{MAP}, \hat{\xi}_{MAP}) 
	=& \underset{\mu, \xi}{\arg \max} \mathbb{P}(\mu, \xi \vert D, K) \\
	=& \underset{\mu, \xi}{\arg \max} \mathbb{P}(\log(\mu) \log(\xi) \vert D, K) \\
	=& \underset{\mu, \xi}{\arg \max} \mathbb{P}(D \vert \log(\mu), \log(\xi), K) \\
	=& \underset{\mu, \xi}{\arg \max} \frac{\sqrt{\mu^{n_f} \xi^n}}	{\sqrt{\det H}} 				\exp(- \mathcal{J}(\hat{w}_{MAP},\hat{b}_{MAP})) \\
	=& \underset{\mu, \xi}{\arg \max} \frac{\sqrt{\mu^{n_f} \xi^n}}	{\sqrt{n \mu^{n_f - 			N_{eff}} \xi \underset{i=1}{\overset{N_{eff}}{\prod}}(\mu + 			\xi 				\lambda_i)}} \exp(- \mathcal{J}(\hat{w}_{MAP},\hat{b}_{MAP})) \\
	=& \underset{\mu, \xi}{\arg \max} \frac{\sqrt{\mu^{N_{eff}} \xi^{n-1}}}	{\sqrt{ 				\underset{i=1}{\overset{N_{eff}}{\prod}}(\mu + 	\xi	\lambda_i)}} \exp(- 					\mathcal{J}(\hat{w}_{MAP},\hat{b}_{MAP})) \\
	=& \underset{\mu, \xi}{\arg \min}  \hspace{0.2cm} \mathcal{J}(\hat{w}_{MAP}, 			   	 	\hat{b}_{MAP}) + \frac{1}{2}\sum_{i=1}^{N_{eff}}\log(\mu + \xi \lambda_i)\\
	 & \hspace{0.8cm}- \frac{N_{eff}}{2}\log(\mu) - \frac{n -1 }{2}\log(\xi),
\end{align*}

was uns (i) liefert. (ii) erhalten wir direkt durch partielles Ableiten.

\end{proof}

Dieses Minimierungsproblem ermöglicht es die Wahl der Hyperparameter mit Hilfe eines rigorosen Verfahrens zu treffen, statt durch "Tuning per Hand" und verschiedenen anderen heuristischen Methoden. Um das Zielfunktional aufzustellen benötigt man die Eigenwerte $\lambda_i$ der zentrierten Kernelmatrix. Es fällt auf, dass die größe der zentrierten Kernelmatrix mit der Größe des Datenvektors $D$ wächst, so dass bei großen Datenmengen extrem viele Eigenwerte auftreten können. In der Praxis wird dem begegnet, indem man approximativ die größten Eigenwerte berechnet und so das Zielfunktional annähert. Dies geschieht beispielsweise, sogar bei unendlichdimensionalem Feature-Space, mit einem Expectation-Maximization-Algorithmus, siehe etwa \cite{Kernel_PCA}. Für umfassende Details zur Spektraltheorie von Kernen im Rahmen des Machine Learning verweisen wir auf \cite{spectral_kernel}.

%Algorithmen zur Lösung???

%SKALARES PROBLEM?


Nachdem wir eine mögliche Antwort auf die Frage der Auswahl von Hyperparametern gegeben haben, wollen wir uns mit der auf der nächst höheren Ebene liegenden Frage der Kernelselektion beschäftigen. Im Allgemeinen ist die Wahl des Kernels zum durchführen des Kerneltricks im Rahmen der nichtlinearen Trennung mittels LS-SVM höchst nicht-trivial. Wir verwenden, nach \cite{LS-SVM}, den Ansatz die Kernel $K_i$ und die jeweils zugehörige LS-SVM mit ihren Parametern $w_i, b_i, \mu_i, \xi_i$ als verschiedene Modelle zu betrachten, und Modellauswahl traditionell bayesianischer Art durchzuführen. Für eine umfassende Einführung in Techniken der Bayesianischen Modellselektion empfehlen wir \cite{Bayes_model_selection}. Im folgenden Stellen wir den recht simplen Ansatz von Gestel, Suykens et. al. vor.

Ein Ansatz der Bayesianische Modelselektion basiert auf einem Ranking der Kernel durch die zugehörige A Posteriori Modellevidenz $\mathbb{P}(K_i\vert D)$. Wir beschränken uns hier auf die Auswahl unter endlich vielen Kernen $\{K_i\}_{i=1,...,m}$. Die Auswahl wird durch die Wahl des Kernels mit der größten Evidenz $\mathbb{P}(K_i\vert D)$ getätigt. Da wir nur endlich viele Kernel zur Auswahl stellen, läuft dies auf ein heuristisches Auswählen und sortieren hinaus.

Ähnlich wie in den vorangegangenen Betrachtungen platzieren wir A Priori Verteilungen, diesmal auf die Kernel $K_I$ selber. Dabei nehmen wir an:

\begin{itemize}
	\item Platziere (diskrete) a priori Verteilung auf die Kerne $K_i$:
		\begin{center}
			$\mathbb{P}(K_i) = p_i, \sum_{i=1}^{m} \mathbb{P}(K_i) = 1$
		\end{center}
	\item Nehme eine Gleichverteilung an, d.h. $p_i = p_j$ für alle $i,j = 1,...,M$.
	\item Seien die Hyperparameter $\mu_i, \xi_i$ der jeweiligen Models mit Kern $K_i$ 				  stochastisch unabhängig und haben folgende Verteilung:
		  \begin{center}
				$\log(\mu_i) \sim \mathcal{N}(0, \sigma_{\mu_i}^2), \log(\xi_i) \sim 						\mathcal{N}(0, \sigma_{\xi_i}^2)$ 
		  \end{center}
	\item $\sigma_{\mu_i}^2, \sigma_{\xi_i}^2 \rightarrow \infty$ für alle $i = 1,...,M				   $.
\end{itemize}

Es ist auch möglich ganze Familien von Kerneln zu betrachten, und unter diesen zu Selektieren. Ein Beispiel wäre die Wahl des geeigneten Kernelparameters eines RBF-Kerns. Diese Fragestellung führt jedoch zu nicht-trivialen Minimierungsproblem in dem Sinne, dass keine einfache Heuristik zu Wahl des optimalen Kernels exisitiert.

Der folgende Satz gibt als weiteres Hauptresultat an, wie man die Evidenzen $\mathbb{P}(K_i \vert D)$ in Proportionalität berechnen kann, und somit Kernel selektieren kann.


\begin{theorem}[Berechnung der Evidenz zur Kernelselektion]
Seien die obigen Annahmen erfüllt. Weiterhin sei ein Datenvektor $D$ gegeben, so dass die A Posteriori Verteilungen $\mathbb{P}(\log(\mu),\log(\xi) \vert D, K_i)$ eine positiv definite Kovarianzmatrix besitzt. Dann gilt:

\begin{align*}
	\mathbb{P}(K_i \vert D) 
	&\propto \mathbb{P}(D \vert K_i)\\
 	&\propto \mathbb{P}(D \vert \log(\hat{\mu}_{i_{MAP}}), \log(\hat{\xi}_{i_{MAP}}), 			K_i)\frac{\sigma_{\mu_i\vert D} \sigma_{\xi_i\vert D}}{\sigma_{\mu_i} 						\sigma_{\xi_i}}\\
	&\propto \sqrt{\frac{\hat{\mu}^{N_{eff}}_{i_{MAP}} \hat{\xi}^{n-1}_{i_{MAP}}}				{(\gamma_{eff}-1)(n - \gamma_{eff}) \prod_{j=1}^{N_{eff}}(\hat{\mu}_{i_{MAP}} + 			\hat{\xi}_{i_{MAP}}\lambda_j)}}
\end{align*}

wobei $\gamma_{eff} = 1 + \sum_{j=1}^{N_{eff}}\frac{\hat{\xi}_{i_{MAP}}\lambda_j}{\hat{\mu}_{i_{MAP}} + \hat{\xi}_{i_{MAP}}\lambda_j}$.

\end{theorem}

\begin{proof}
Die erste Proportionalität ergibt sich durch die Gleichverteilungsannahme an die $K_i$ und Definition des Posteriors:

\begin{center}
	$\mathbb{P}(K_i \vert D) = \frac{\mathbb{P}(D\vert K_i)\mathbb{P}(K_i)}{\mathbb{P}			(D)}
	\propto \mathbb{P}(D \vert K_i)$
\end{center}
Für den Rest verweisen wir auf \cite{LS-SVM}.
\end{proof}

Der Faktor $\frac{\sigma_{\mu_i\vert D} \sigma_{\xi_i\vert D}}{\sigma_{\mu_i} 						\sigma_{\xi_i}}$ wird auch mit Occam's Faktor bezeichnet, benannt nach dem mittelalterlichen Philosophen Wilhelm von Ockham, auf den auch der bekannte Begriff der Occam'schen Rasierklinge zurückgeht. Bemerkenswert an der obigen proportionalen Darstellung ist, dass sobald man MAP-Schätzer und Eigenwerte aus vorangehender Analyse gebildet hat, die Modellevidenz im Proportionalen sofort berechnet werden kann. Bei Optimierung hinsichtlich ganzer Kernelfamilien ließe sich diese Proportionalität als Ansatz nutzen um ein Minimierungsproblem herzuleiten.






\nocite{*}
\bibliographystyle{plain}
\bibliography{papers}

\end{document}
