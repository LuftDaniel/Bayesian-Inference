\documentclass{report}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{enumerate}% schicke Nummerierung
\usepackage{graphicx}
\usepackage[english, ngerman]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage{bigdelim}
\usepackage{multirow}
\usepackage{dsfont, hyperref}
\usepackage{cite}
\usepackage[nottoc]{tocbibind}
\usepackage{empheq}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{lipsum}
\geometry{a4paper,left=40mm,right=30mm, top=5cm, bottom=5cm} 



\newtheoremstyle{linebreak}   % name
{3pt}                         % Space above
{3pt}                         % Space below
{}                            % Body font
{}                            % Indent amount 1
{\bfseries}                   % Theorem head font
{\newline}                    % Punctuation after theorem head
{.5em}                        % Space after theorem head 2
{}                            % Theorem head spec (can be left empty, meaning ‘normal’)
\theoremstyle{linebreak}
\newtheorem{defi}{Definition}%[chapter]
\newtheorem{satz}[defi]{Satz}
\newtheorem{theorem}[defi]{Theorem}
\newtheorem{propo}[defi]{Proposition}
\newtheorem{lemma}[defi]{Lemma}
\newtheorem{cor}[defi]{Korollar}
\newtheorem{bem}[defi]{Bemerkung}
\newtheorem{bsp}[defi]{Beispiel}
\newtheorem{folg}[defi]{Folgerung}
%bemerkungen oder Fließtext???
 \newcommand{\newln}{\\&\quad\quad{}}

\renewenvironment{abstract}
 {\small
  \begin{center}
  \bfseries \abstractname\vspace{-.5em}\vspace{0pt}
  \end{center}
  \list{}{%
    \setlength{\leftmargin}{12mm}% <---------- CHANGE HERE
    \setlength{\rightmargin}{\leftmargin}%
  }%
  \item\relax}
 {\endlist}




\begin{document}
\title{Seminar über Numerische Methoden im Machine Learning \\ Bayesianische Inferenz}

\author{Daniel Luft & Fabian Gernandt \\ Prof. Dr. V. Schulz}
%\maketitle

%\tableofcontents 
\selectlanguage{ngerman}
\begin{abstract}
In diesem Paper möchten wir eine kurze Einführung in die Bayes Theorie im Rahmen des Machine Learnings geben, welche den Anspruch hat für Leser mit nur geringen Kenntnissen der Statistik und Wahrscheinlichkeitstheorie lesbar zu sein. Wir nutzen die Bayes Theorie um alternative numerische Problemstellungen zu "klassischen" Verfahren des Machine Learning zu erzeugen. Exemplarisch führen wir aufbauend auf der linearen Regression in die Bayesianische Regression ein, und erhalten die regularisierten Verfahren "Ridge Regression" und "LASSO". Weiterhin greifen wir die Klassifizierungstechnik der Support Vector Machine (SVM) auf und geben eine Herleitung der sogenannten "Least-Squares-SVM" (LS-SVM) an. Weiterführend werden Möglichkeiten zur Hyperparameterschätzung, sowie der Selektion von Kernen in diesem Modell erarbeitet. Diese Techniken münden abschließend gemeinsam in einen Algorithmus zur Lösung der LS-SVM bei mehreren konkurrierenden Kernen.
\end{abstract}
\vspace{1cm}


\subsection{Einführung in die Bayes Theorie}

Das Maschinelle Lernen nimmt in dem Zeitalter der Digitalen Revolution eine zunehmend zentrale Rolle ein. Schon jetzt basieren zahlreiche Produkte von global Playern wie Google, Facebook, aber auch Kalashnikov und Co. auf Techniken des Maschinellen Lernens. Zu diesen Techniken zählen Klassifizierungsmethoden wie die Support Vector Machine (SVM) und sogenannte Neural Networks. Eines ihrer charakteristischen Merkmale ist die Verwendung großer Datensätze (Big Data), was zu interessanten Fragestellungen im Bereich der Mathematik und Statistik, aber auch der Informatik führt. Wir möchten in diesem Paper eine mathematisch-statistische Perspektive auf gewisse Problemklassen einnehmen, um hieraus einen erweiterten Zugang zu "klassischen" numerischen Problemstellungen des Machine Learning zu erhalten.

%hier vielleicht etwas mehr zur Bayes Theorie?
%Notation D= ((x,y),...)? Einheitlich? Modelltraining erwähnen oder Verweis?
%Verweise zur Relevanz/ Quellen?

Ziel ist es, bestimmte Sachverhalte mit Hilfe von Modellgleichungen abzubilden. Diese Modelle bestehen in der Regel aus Inputdaten $x \in \mathbb{R}^m$, einem Output $y \in \mathbb{R}^p$, sowie von Modellparametern, welche wir mit $\theta \in \mathbb{R}^k$ bezeichnen werden. Weiterhin werden wir Datensätze $D = (d_1, \cdots, d_n), d_i \in \mathbb{R}^l$ verwenden, um unsere Modelle zu trainieren. 

%Diagramm? Lineare Regression Vorwegnehmen, um exemplarisch zu zeigen was gemeint ist?

Hierzu wählen wir den statistischen Ansatz der sogenannten Bayes Theorie. Diese Theorie verwendet Verteilungsannahmen an Parameter $\theta$ des Modells. Solche Annahmen ermöglichen eine umfangreiche wahrscheinlichkeitstheoretische Behandlung des Modells.

Zunächst einigen wir uns auf folgende Notation von Verteilungen und Wahrscheinlichkeiten: \\
Sei $(\Omega, \mathcal{A}, \mathbb{P})$ ein hinreichend großer Wahrscheinlichkeitsraum, denn wir im Folgenden immer im Hintergrund voraussetzen werden. Weiterhin sei $\theta: \Omega \rightarrow \mathbb{R}^k$ eine Zufallsvariable. Dann bezeichnen wir mit
$\mathbb{P}(\theta) := \mathbb{P}^\theta = \mathbb{P}(\{ \theta \in \cdot \})$
das Bildmaß der Zufallsvariable $\theta$, also Ihre Verteilung.
Wir möchten hier bemerken, dass es keinen Sinn macht nach der Wahrscheinlichkeit von $\theta$ zu fragen, sondern lediglich von der Wahrscheinlichkeit dass $\theta$ bestimmte Werte annimmt. Dies ist wichtig, um Missverständnisse zu vermeiden.

Wir möchten nun in die Grundbegriffe der Bayes Theorie einführen. 
%Zitat woher die Definitionen kommen.
Die zentralen Objekte der Bayes Theorie sind die A Priori Verteilung (Prior) und die A Posteriori Verteilung (Posterior), welche durch den Satz von Bayes in Verbindung stehen. 

\begin{defi}[A Priori Verteilung]
	Sei der Modellparameter $\theta$ mit Werten im $\mathbb{R}^k$ eine Zufallsvariable. 		\\
	Dann heißt seine Verteilung $\mathbb{P}(\theta)$ \textit{A Priori Verteilung}, kurz 		\textit{Prior}. 
\end{defi}
%Quelle zur Bayes Theorie

Der Prior als Wahrscheinlichkeitsverteilung muss vor dem weiteren Arbeiten mit einem Bayesianischen Modell durch eine Annahme festgelegt werden. Es ist für sich nicht klar welcher Prior für ein gegebenes Modell sinnvoll ist, was zur Gefahr widersprüchlicher oder hinderlicher Annahmen führt. Deshalb ist vor dem Platzieren eines Priors eine gründliche Überlegung über Eigenschaften der Parameter wichtig. Verschiedene Priors führen zu verschiedenen Modellen und somit zu verschiedenen Problemklassen, weshalb auch eine entgegengesetzte Betrachtung möglich ist, bei der bei gegebener Problemklasse der zugrunde liegende Prior gesucht ist. Wir wählen den anderen Ansatz, und konstruieren Probleme bei gegebenem Prior.

Bevor wir die A Posteriori Verteilung einführen, erinnern wir an den Satz von Bayes.

\begin{theorem}[Satz von Bayes (Bayes 1763, Laplace 1812)]
	Sei $(\Omega, \mathcal{A}, \mathbb{P})$ ein Wahrscheinlichkeitsraum, seien 
	$A,B \in \mathcal{A}$ Ereignisse. Dann gilt für die bedingten Wahrscheinlichkeiten
	\begin{center}
		$\mathbb{P}(A\vert B) = \frac{\mathbb{P}(B \vert A) \mathbb{P}(A)}{\mathbb{P}				(B)}$.
	\end{center}
	Falls $\mathbb{P}(B) = 0$, so definieren wir den rechten Ausdruck als $0$.
\end{theorem}

\begin{proof}
	Mit Definition einer bedingten Wahrscheinlichkeit erhält man
	\begin{center}
		$\frac{\mathbb{P}(B \vert A) \mathbb{P}(A)}{\mathbb{P}(B)}
		= \frac{\frac{\mathbb{P}(B \cap A)}{\mathbb{P}(A)} \mathbb{P}(A)}{\mathbb{P}(B)}
		= \frac{\mathbb{P}(B \cap A)}{\mathbb{P}(B)}
		= \mathbb{P}(A\vert B)$.
	\end{center}
\end{proof}

Gegeben eines Priors möchte man nun die statistischen Daten eines Modells in die Information über die Parameter miteinfliessen lassen. Die hierzu folgende Definition ist in Anlehnung an den Satz von Bayes motiviert.

\begin{defi}[A Posteriori Verteilung]
	Sei $\theta$ ein Modellparameter mit A Priori Verteilung $\mathbb{P}(\theta)$, $D = 		(d_1, \cdots, d_n)$ ein Datenvektor. Dann definieren wir die \textit{A Posteriori 			Verteilung}, kurz \textit{Posterior}, durch
	\begin{center}
		$\mathbb{P}(\theta \vert D) := \frac{\mathbb{P}(D \vert \theta) 
		\mathbb{P}(\theta)}{\mathbb{P}(D)}$,
	\end{center}
	wobei $\mathbb{P}(D) = \int \mathbb{P}(D \vert \theta) \mathbb{P}(\theta)$.
\end{defi}

In dem Posterior sind die kombinierte statistische Information der Annahmen, sowie der beobachteten Daten, über $\theta$ enthalten. Den Prozess der Berechnung des Posteriors wird oft auch \texit{Training} genannt, in diesem Kontext nennt man $D$ oft auch \textit{Trainingsdaten}. 
%Variational Bayes und berechnung schwierig anmerken + likelihood und prior

%verweis zu informationsgeometrie, Herr Schulz findet das bestimmt super

\nocite{*}
\bibliographystyle{plain}
\bibliography{papers}

\end{document}